<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>NOW — COMPUTER MUSIC</title>
</head>
<body bgcolor="#E8F1F8" text="#000000" link="#0000EE" vlink="#551A8B" alink="#FF0000"
      style="font-family:'Times New Roman', Times, serif;">

<table width="760" align="center" cellpadding="10" cellspacing="0" border="1">
  <tr>
    <td>
      <p>
        <a href="index.html">HOME</a> |
        <a href="philosophies.html">PHILOSOPHIES</a> |
        <a href="timeline.html">TIMELINE</a> |
        <a href="tech.html">TECH</a> |
        <a href="midi-max.html">MIDI &amp; MAX/MSP</a>
      </p>
      <hr>

      <h1>NOW</h1>

      <h2>ENSEMBLES THAT LOOK LIKE SOFTWARE PROJECTS</h2>
      <p>
        <a href="https://en.wikipedia.org/wiki/Stanford_Laptop_Orchestra">Stanford Laptop Orchestra (SLOrk)</a> is noted as being based
        out of Stanford's CCRMA, founded by <a href="https://en.wikipedia.org/wiki/Ge_Wang">Ge Wang</a>, and presented as a
        major computer music ensemble.
      </p>
      <p>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Ge_Wang.jpg/320px-Ge_Wang.jpg" alt="Ge Wang" width="240">
        <br><small>Image: Ge Wang — CC0 (via Wikimedia Commons)</small>
      </p>
      </p>

      <h2>NEW INPUTS → NEW INSTRUMENTS</h2>
      <p>
        Brian Ellis / sounds.pink is described as using pose/emotion/body
        recognition to convert performance into audio data (MIDI).
      </p>

      <h2>AI IN MUSIC (AS LISTED)</h2>
      <ul>
        <li><b>Sample indexing:</b> ML/language models tagging your audio (e.g., "piano", mood, timbre).</li>
        <li><b>Timbre transformation:</b> networks trained to transform one sound into another (e.g., piano → choir).</li>
      </ul>

      <h2>EXPANDING INTO THE VISUAL</h2>
      <p>
        <a href="https://en.wikipedia.org/wiki/TouchDesigner">TouchDesigner</a> and <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a>
        enable generating images that react to sound in real-time through Python APIs.
      </p>
      <p>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/NightCitySphere_%28SD1.5%29.jpg/480px-NightCitySphere_%28SD1.5%29.jpg" alt="Stable Diffusion example" width="320">
        <br><small>Image: Stable Diffusion example — CC BY 4.0 (via Wikimedia Commons)</small>
      </p>
      </p>

      <hr>

      <h2>THE QUESTIONS (ENDGAME)</h2>
      <p>
        The core inquiry is both intellectual and aesthetic: what does it mean to make computer music? Can
        algorithms be part of composition? What are the ethical implications of AI in music?
      </p>
      <p>
        A productive stance holds that composers and machines can coexist; "analogue" workflows carry no inherent moral superiority; the depth
        of digital processing opens possibilities for experimentation; and computational thinking mirrors the scientific method
        (hypothesise → abstract → experiment → record evidence).
      </p>

      <p><small><a href="index.html">← back</a></small></p>
    </td>
  </tr>
</table>

</body>
</html>
