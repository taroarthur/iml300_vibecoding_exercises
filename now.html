<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>NOW — COMPUTER MUSIC</title>
</head>
<body bgcolor="#FFFFFF" text="#000000" link="#0000EE" vlink="#551A8B" alink="#FF0000"
      style="font-family:'Times New Roman', Times, serif;">

<table width="760" align="center" cellpadding="10" cellspacing="0" border="1">
  <tr>
    <td>
      <p>
        <a href="index.html">HOME</a> |
        <a href="philosophies.html">PHILOSOPHIES</a> |
        <a href="timeline.html">TIMELINE</a> |
        <a href="tech.html">TECH</a> |
        <a href="midi-max.html">MIDI &amp; MAX/MSP</a>
      </p>
      <hr>

      <h1>NOW</h1>

      <h2>ENSEMBLES THAT LOOK LIKE SOFTWARE PROJECTS</h2>
      <p>
        [Stanford Laptop Orchestra (SLOrk)](chatgpt://generic-entity?number=17) is noted as being based
        out of Stanford’s CCRMA, founded by [Ge Wang](chatgpt://generic-entity?number=18), and presented as a
        major computer music ensemble.  [oai_citation:25‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)
      </p>

      <h2>NEW INPUTS → NEW INSTRUMENTS</h2>
      <p>
        [Brian Ellis](chatgpt://generic-entity?number=19) / sounds.pink is described as using pose/emotion/body
        recognition to convert performance into audio data (MIDI).  [oai_citation:26‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)
      </p>

      <h2>AI IN MUSIC (AS LISTED)</h2>
      <ul>
        <li><b>Sample indexing:</b> ML/language models tagging your audio (e.g., “piano”, mood, timbre).  [oai_citation:27‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)</li>
        <li><b>Timbre transformation:</b> networks trained to transform one sound into another (e.g., piano → choir).  [oai_citation:28‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)</li>
      </ul>

      <h2>EXPANDING INTO THE VISUAL</h2>
      <p>
        The deck points to [TouchDesigner](chatgpt://generic-entity?number=20) and [Stable Diffusion](chatgpt://generic-entity?number=21),
        with an example of calling an API via Python to generate images reacting to sound live.  [oai_citation:29‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)
      </p>

      <hr>

      <h2>THE QUESTIONS (ENDGAME)</h2>
      <p>
        Your closing slide frames it as intellectual + aesthetic: what it means to make computer music, whether
        algorithms can be part of composition, and ethics around AI.  [oai_citation:30‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)
      </p>
      <p>
        And your stated stance: composers can coexist with machines; “analogue” isn’t morally superior; the depth
        of digital processing invites experimentation; computational thinking resembles the scientific method
        (hypothesise → abstract → experiment → record evidence).  [oai_citation:31‡d'Aronville_IML288_ArtistPresentation_W4.pdf](sediment://file_00000000018471f5a21ca33fb9ae97ae)
      </p>

      <p><small><a href="index.html">← back</a></small></p>
    </td>
  </tr>
</table>

</body>
</html>
